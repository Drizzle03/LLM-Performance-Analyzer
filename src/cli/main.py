"""
CLI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ - Claude API vs HyperClovaX API ì„±ëŠ¥ ë¹„êµ ë„êµ¬
"""
import asyncio
import os
from typing import List, Dict, Any, Optional
from datetime import datetime

try:
    import click
except ImportError:
    click = None

from ..api.claude_api import ClaudeAPI
from ..api.hyperclova_api import HyperClovaAPI
from ..api.openai_api import OpenAIAPI
from ..api.gemini_api import GeminiAPI
from ..api.grok_api import GrokAPI
from ..metrics.evaluator import MetricsEvaluator
from ..utils.config import settings


class APIComparator:
    """API ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ë¹„êµ ë„êµ¬ ì´ˆê¸°í™”"""
        self.apis = {}
        self.evaluator = MetricsEvaluator()
        
        # ëª¨ë“  API ì´ˆê¸°í™” ì‹œë„
        api_classes = {
            "Claude": ClaudeAPI,
            "ChatGPT": OpenAIAPI,
            "Gemini": GeminiAPI,
            "Grok": GrokAPI,
            "HyperClovaX": HyperClovaAPI
        }
        
        for name, api_class in api_classes.items():
            try:
                self.apis[name] = api_class()
                print(f"âœ… {name} API ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {name} API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        if not self.apis:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ APIê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"ğŸ¯ ì´ {len(self.apis)}ê°œ AI ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥: {', '.join(self.apis.keys())}")
    
    async def test_question_generation(self, user_responses: List[str], contexts: Optional[List[str]] = None) -> Dict[str, Any]:
        """ì§ˆë¬¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
        if contexts is None:
            contexts = [""] * len(user_responses)
        
        results = {api_name.lower(): [] for api_name in self.apis.keys()}
        failed_apis = set()  # ì‹¤íŒ¨í•œ API ì¶”ì 
        
        for i, (user_response, context) in enumerate(zip(user_responses, contexts)):
            test_id = f"question_test_{i+1}"
            
            # ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ API í…ŒìŠ¤íŠ¸
            for api_name, api_instance in self.apis.items():
                # ì´ë¯¸ ì‹¤íŒ¨í•œ APIëŠ” ìŠ¤í‚µ
                if api_name in failed_apis:
                    continue
                    
                try:
                    result = await api_instance.generate_questions(user_response, context)
                    
                    if not result.error:
                        quality_metrics = self.evaluator.evaluate_question_quality(
                            result.content, user_response, context
                        )
                        performance_metrics = self.evaluator.calculate_performance_metrics(
                            result.response_time, result.tokens_used, result.cost
                        )
                        
                        self.evaluator.add_comparison_result(
                            test_id, api_name, "question_generation",
                            user_response, result.content,
                            quality_metrics, performance_metrics
                        )
                        
                        results[api_name.lower()].append({
                            "content": result.content,
                            "response_time": result.response_time,
                            "tokens": result.tokens_used,
                            "cost": result.cost,
                            "quality_score": quality_metrics.overall_score
                        })
                    else:
                        # API ì˜¤ë¥˜ ì‹œ ê°„ê²°í•œ ë©”ì‹œì§€
                        error_msg = self._get_simplified_error(result.error)
                        print(f"âŒ {api_name}: {error_msg}")
                        failed_apis.add(api_name)
                        
                except Exception as e:
                    # ì˜ˆì™¸ ë°œìƒ ì‹œ ê°„ê²°í•œ ë©”ì‹œì§€
                    error_msg = self._get_simplified_error(str(e))
                    print(f"âŒ {api_name}: {error_msg}")
                    failed_apis.add(api_name)
        
        return results
    
    def _get_simplified_error(self, error_msg: str) -> str:
        """ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ê°„ê²°í•˜ê²Œ ë³€í™˜"""
        error_str = str(error_msg).lower()
        
        if "quota" in error_str or "429" in error_str:
            return "í• ë‹¹ëŸ‰ ì´ˆê³¼ (ë¬´ë£Œ í‹°ì–´ í•œë„)"
        elif "timeout" in error_str or "timed out" in error_str:
            return "ìš”ì²­ íƒ€ì„ì•„ì›ƒ"
        elif "401" in error_str or "unauthorized" in error_str:
            return "API í‚¤ ì¸ì¦ ì˜¤ë¥˜"
        elif "404" in error_str or "not found" in error_str:
            return "ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
        elif "connection" in error_str:
            return "ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜"
        else:
            # ê¸´ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ì§§ê²Œ ìë¥´ê¸°
            return error_str[:100] + "..." if len(error_str) > 100 else error_str
    
    async def test_report_generation(self, report_scenarios: List[Any]) -> Dict[str, Any]:
        """ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸"""
        results = {api_name.lower(): [] for api_name in self.apis.keys()}
        
        for i, scenario in enumerate(report_scenarios):
            test_id = f"report_test_{i+1}"
            
            # ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ API í…ŒìŠ¤íŠ¸
            for api_name, api_instance in self.apis.items():
                try:
                    result = await api_instance.generate_report(
                        scenario.conversation_history, scenario.prompt
                    )
                    
                    if not result.error:
                        quality_metrics = self.evaluator.evaluate_report_quality(
                            result.content, scenario.conversation_history, scenario.prompt
                        )
                        performance_metrics = self.evaluator.calculate_performance_metrics(
                            result.response_time, result.tokens_used, result.cost, None
                        )
                        
                        self.evaluator.add_comparison_result(
                            test_id, api_name, "report_generation",
                            scenario.prompt, result.content,
                            quality_metrics, performance_metrics
                        )
                        
                        results[api_name.lower()].append({
                            "content": result.content,
                            "response_time": result.response_time,
                            "tokens": result.tokens_used,
                            "cost": result.cost,
                            "quality_score": quality_metrics.overall_score
                        })
                    else:
                        print(f"âŒ {api_name} API ì˜¤ë¥˜ (í…ŒìŠ¤íŠ¸ {i+1}): {result.error}")
                        
                except Exception as e:
                    print(f"âŒ {api_name} API ì˜ˆì™¸ (í…ŒìŠ¤íŠ¸ {i+1}): {e}")
        
        return results
    
    def print_summary(self, question_results: Dict[str, Any], report_results: Dict[str, Any]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n=== ğŸ“Š AI ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ===")
        
        # ì§ˆë¬¸ ìƒì„± ê²°ê³¼
        print("\nğŸ¤” ì§ˆë¬¸ ìƒì„± ì„±ëŠ¥ ë¹„êµ:")
        for api_name in self.apis.keys():
            api_key = api_name.lower()
            if api_key in question_results and question_results[api_key]:
                results = question_results[api_key]
                avg_time = sum(r["response_time"] for r in results) / len(results)
                avg_quality = sum(r["quality_score"] for r in results) / len(results)
                avg_cost = sum(r["cost"] for r in results) / len(results)
                print(f"  {api_name:12}: ì‘ë‹µì‹œê°„ {avg_time:5.2f}ì´ˆ, í’ˆì§ˆ {avg_quality:.2f}/5.0, ë¹„ìš© ${avg_cost:.4f}")
        
        # ë³´ê³ ì„œ ìƒì„± ê²°ê³¼ (í–¥í›„ êµ¬í˜„)
        if any(report_results.values()):
            print("\nğŸ“‹ ë³´ê³ ì„œ ìƒì„± ì„±ëŠ¥ ë¹„êµ:")
            for api_name in self.apis.keys():
                api_key = api_name.lower()
                if api_key in report_results and report_results[api_key]:
                    results = report_results[api_key]
                    avg_time = sum(r["response_time"] for r in results) / len(results)
                    avg_quality = sum(r["quality_score"] for r in results) / len(results)
                    avg_cost = sum(r["cost"] for r in results) / len(results)
                    print(f"  {api_name:12}: ì‘ë‹µì‹œê°„ {avg_time:5.2f}ì´ˆ, í’ˆì§ˆ {avg_quality:.2f}/5.0, ë¹„ìš© ${avg_cost:.4f}")
        
        # ì„±ëŠ¥ ë­í‚¹ ì¶œë ¥
        self._print_performance_ranking(question_results)
    
    def _print_performance_ranking(self, question_results: Dict[str, Any]):
        """ì„±ëŠ¥ ìˆœìœ„ ì¶œë ¥"""
        if not question_results:
            return
        
        rankings = []
        for api_name in self.apis.keys():
            api_key = api_name.lower()
            if api_key in question_results and question_results[api_key]:
                results = question_results[api_key]
                avg_time = sum(r["response_time"] for r in results) / len(results)
                avg_quality = sum(r["quality_score"] for r in results) / len(results)
                avg_cost = sum(r["cost"] for r in results) / len(results)
                
                # ì¢…í•© ì ìˆ˜ ê³„ì‚° (í’ˆì§ˆ ì¤‘ì‹¬, ì†ë„ì™€ ë¹„ìš© ê³ ë ¤)
                composite_score = avg_quality * 0.6 + (5 - min(avg_time, 5)) * 0.2 + (1 - min(avg_cost, 1)) * 0.2
                
                rankings.append({
                    "name": api_name,
                    "quality": avg_quality,
                    "speed": avg_time,
                    "cost": avg_cost,
                    "composite": composite_score
                })
        
        if rankings:
            print("\nğŸ† ì¢…í•© ì„±ëŠ¥ ìˆœìœ„:")
            rankings.sort(key=lambda x: x["composite"], reverse=True)
            
            for i, ranking in enumerate(rankings, 1):
                medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}ìœ„"
                print(f"  {medal} {ranking['name']:12} (ì¢…í•©: {ranking['composite']:.2f}, í’ˆì§ˆ: {ranking['quality']:.2f}, ì†ë„: {ranking['speed']:.2f}ì´ˆ, ë¹„ìš©: ${ranking['cost']:.4f})")
    
    def save_results(self, filename: Optional[str] = None):
        """ê²°ê³¼ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"api_comparison_results_{timestamp}"
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(settings.results_dir, exist_ok=True)
        
        json_file = os.path.join(settings.results_dir, f"{filename}.json")
        self.evaluator.export_results(json_file, "json")


def get_test_data_by_category(category: str = "all", limit: int = 3):
    """ì¹´í…Œê³ ë¦¬ë³„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë°˜í™˜"""
    from ..tests.test_scenarios import TestScenarios
    
    if category == "all":
        scenarios = TestScenarios.get_question_generation_scenarios()
    else:
        scenarios = TestScenarios.get_scenarios_by_category(category)
    
    # ì œí•œëœ ìˆ˜ë§Œí¼ë§Œ ì„ íƒ
    selected_scenarios = scenarios[:limit] if scenarios else []
    
    user_responses = [scenario.user_input for scenario in selected_scenarios]
    contexts = [scenario.context for scenario in selected_scenarios]
    names = [scenario.name for scenario in selected_scenarios]
    
    return user_responses, contexts, names


def get_available_categories():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
    from ..tests.test_scenarios import TestScenarios
    scenarios = TestScenarios.get_question_generation_scenarios()
    categories = list(set(scenario.category for scenario in scenarios))
    return sorted(categories)


def get_sample_test_data():
    """ê¸°ë³¸ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    user_responses, contexts, _ = get_test_data_by_category("job_preparation", 3)
    return user_responses, contexts


async def run_comparison_test(category: str = "all", count: int = 3):
    """ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ API ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    # ì„¤ì • ê²€ì¦
    if not settings.validate_api_keys():
        print("âŒ API í‚¤ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # ë¹„êµ ë„êµ¬ ì´ˆê¸°í™”
    comparator = APIComparator()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    user_responses, contexts, names = get_test_data_by_category(category, count)
    
    if not user_responses:
        print(f"âŒ '{category}' ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ì§ˆë¬¸ ìƒì„± í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤... (ì¹´í…Œê³ ë¦¬: {category}, ì‹œë‚˜ë¦¬ì˜¤ ìˆ˜: {len(user_responses)})")
    
    # ê° ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ ì¶œë ¥
    for i, name in enumerate(names, 1):
        print(f"  {i}. {name}")
    print()
    
    question_results = await comparator.test_question_generation(user_responses, contexts)
    
    print("\nğŸ“Š ê²°ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")
    empty_report_results = {api_name.lower(): [] for api_name in comparator.apis.keys()}
    comparator.print_summary(question_results, empty_report_results)
    
    print("\nğŸ’¾ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")
    comparator.save_results()
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


async def run_report_test(category: str = "start_technique_report", count: int = 1):
    """ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ“‹ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    # ì„¤ì • ê²€ì¦
    if not settings.validate_api_keys():
        print("âŒ API í‚¤ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # ë¹„êµ ë„êµ¬ ì´ˆê¸°í™”
    comparator = APIComparator()
    
    # ë³´ê³ ì„œ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ê°€ì ¸ì˜¤ê¸°
    from ..tests.test_scenarios import TestScenarios
    all_report_scenarios = TestScenarios.get_report_generation_scenarios()
    
    # ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§
    if category != "all":
        report_scenarios = [s for s in all_report_scenarios if s.category == category]
    else:
        report_scenarios = all_report_scenarios
    
    # ê°œìˆ˜ ì œí•œ
    report_scenarios = report_scenarios[:count]
    
    if not report_scenarios:
        print(f"âŒ '{category}' ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë³´ê³ ì„œ ì‹œë‚˜ë¦¬ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤... (ì¹´í…Œê³ ë¦¬: {category}, ì‹œë‚˜ë¦¬ì˜¤ ìˆ˜: {len(report_scenarios)})")
    
    # ê° ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ ì¶œë ¥
    for i, scenario in enumerate(report_scenarios, 1):
        print(f"  {i}. {scenario.name}")
    print()
    
    # ë³´ê³ ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - ê°œì„ ëœ ì˜ˆì™¸ì²˜ë¦¬ ì ìš©
    report_results = {}
    for api_name in comparator.apis.keys():
        report_results[api_name.lower()] = []
    
    failed_apis = set()  # ì‹¤íŒ¨í•œ API ì¶”ì 
    
    for i, scenario in enumerate(report_scenarios):
        print(f"ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ {i+1}: {scenario.name}")
        test_id = f"report_test_{i+1}"
        
        # ê° APIë¡œ ë³´ê³ ì„œ ìƒì„±
        for api_name, api_instance in comparator.apis.items():
            # ì´ë¯¸ ì‹¤íŒ¨í•œ APIëŠ” ìŠ¤í‚µ
            if api_name in failed_apis:
                continue
                
            try:
                # conversation_historyë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
                conversation_text = ""
                for msg in scenario.conversation_history:
                    role = "ì‚¬ìš©ì" if msg["role"] == "user" else "AI"
                    conversation_text += f"{role}: {msg['content']}\n"
                
                # ë³´ê³ ì„œ ìƒì„± ìš”ì²­
                result = await api_instance.generate_report(
                    scenario.conversation_history, scenario.prompt
                )
                
                if not result.error:
                    quality_metrics = comparator.evaluator.evaluate_report_quality(
                        result.content, scenario.conversation_history, scenario.prompt
                    )
                    performance_metrics = comparator.evaluator.calculate_performance_metrics(
                        result.response_time, result.tokens_used, result.cost, None
                    )
                    
                    # ê²°ê³¼ë¥¼ evaluatorì— ì œëŒ€ë¡œ ì €ì¥
                    comparator.evaluator.add_comparison_result(
                        test_id, api_name, "report_generation",
                        conversation_text, result.content,
                        quality_metrics, performance_metrics
                    )
                    
                    report_results[api_name.lower()].append({
                        "content": result.content,
                        "response_time": result.response_time,
                        "tokens": result.tokens_used,
                        "cost": result.cost,
                        "quality_score": quality_metrics.overall_score
                    })
                else:
                    # API ì˜¤ë¥˜ ì‹œ ê°„ê²°í•œ ë©”ì‹œì§€
                    error_msg = comparator._get_simplified_error(result.error)
                    print(f"âŒ {api_name}: {error_msg}")
                    failed_apis.add(api_name)
                    
            except Exception as e:
                # ì˜ˆì™¸ ë°œìƒ ì‹œ ê°„ê²°í•œ ë©”ì‹œì§€
                error_msg = comparator._get_simplified_error(str(e))
                print(f"âŒ {api_name}: {error_msg}")
                failed_apis.add(api_name)
    
    print("\nğŸ“Š ê²°ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")
    empty_question_results = {api_name.lower(): [] for api_name in comparator.apis.keys()}
    comparator.print_summary(empty_question_results, report_results)
    
    print("\nğŸ’¾ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")
    comparator.save_results()
    
    print("\nâœ… ëª¨ë“  ë³´ê³ ì„œ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    if not click:
        print("âŒ click íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install click")
        print("ğŸ”„ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
        asyncio.run(run_comparison_test())
        return
    
    @click.group()
    def cli():
        """Claude API vs HyperClovaX API ì„±ëŠ¥ ë¹„êµ ë„êµ¬"""
        pass
    
    @cli.command()
    @click.option('--category', '-c', default='all', help='í…ŒìŠ¤íŠ¸í•  ì‹œë‚˜ë¦¬ì˜¤ ì¹´í…Œê³ ë¦¬')
    @click.option('--count', '-n', default=3, help='ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ ê°œìˆ˜')
    def test(category, count):
        """API ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        asyncio.run(run_comparison_test(category, count))
    
    @cli.command()
    @click.option('--category', '-c', default='start_technique_report', help='ë³´ê³ ì„œ í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬')
    @click.option('--count', '-n', default=1, help='ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ ê°œìˆ˜')
    def report(category, count):
        """ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        asyncio.run(run_report_test(category, count))
    
    @cli.command()
    def categories():
        """ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì¹´í…Œê³ ë¦¬ ëª©ë¡"""
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬:")
        categories = get_available_categories()
        for i, cat in enumerate(categories, 1):
            print(f"  {i}. {cat}")
        
        print("\nğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ ì‹œë‚˜ë¦¬ì˜¤ ê°œìˆ˜:")
        from ..tests.test_scenarios import TestScenarios
        for cat in categories:
            scenarios = TestScenarios.get_scenarios_by_category(cat)
            print(f"  {cat}: {len(scenarios)}ê°œ")
        
        print("\nğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:")
        print("  python main.py test --category job_preparation --count 5")
        print("  python main.py test -c learning -n 2")
        print("  python main.py test --category all --count 10")
    
    @cli.command()
    def scenarios():
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ëª©ë¡"""
        from ..tests.test_scenarios import TestScenarios
        all_scenarios = TestScenarios.get_question_generation_scenarios()
        
        print(f"ğŸ“ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ({len(all_scenarios)}ê°œ):")
        
        categories = get_available_categories()
        for category in categories:
            cat_scenarios = TestScenarios.get_scenarios_by_category(category)
            print(f"\nğŸ·ï¸ {category.upper()} ({len(cat_scenarios)}ê°œ):")
            for i, scenario in enumerate(cat_scenarios, 1):
                print(f"  {i}. {scenario.name}")
                print(f"     ğŸ“„ {scenario.user_input[:50]}...")
                print(f"     ğŸ¯ {scenario.context}")
    
    @cli.command()
    def info():
        """ì‹œìŠ¤í…œ ì •ë³´ ë° ì„¤ì • í™•ì¸"""
        print("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´")
        print(f"- í…ŒìŠ¤íŠ¸ ì–¸ì–´: {settings.test_language}")
        print(f"- ìµœëŒ€ ì¬ì‹œë„: {settings.max_retries}")
        print(f"- ìš”ì²­ íƒ€ì„ì•„ì›ƒ: {settings.request_timeout}ì´ˆ")
        print(f"- ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {settings.results_dir}")
        
        # API í‚¤ ê²€ì¦ (validate_api_keysì—ì„œ ì¶œë ¥)
        settings.validate_api_keys()
    
    cli()


if __name__ == "__main__":
    main() 